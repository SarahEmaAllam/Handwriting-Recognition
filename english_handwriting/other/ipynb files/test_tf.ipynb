{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T18:08:33.597857Z",
     "start_time": "2023-06-10T18:08:30.250331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:38:36.568581Z",
     "start_time": "2023-06-10T20:38:36.529631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreeaioanatudor/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 7173451646684226902\n"
     ]
    }
   ],
   "source": [
    "path = '../../data/preprocessed_data/test'\n",
    "data = tf.data.Dataset.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution is not enabled.\n"
     ]
    }
   ],
   "source": [
    "if tf.config.run_functions_eagerly(True):\n",
    "    print(\"Eager execution is enabled.\")\n",
    "else:\n",
    "    print(\"Eager execution is not enabled.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:36:14.488687Z",
     "start_time": "2023-06-10T20:36:14.483516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:36:37.255913Z",
     "start_time": "2023-06-10T20:36:37.244108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 7173451646684226902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ds = data.take(1)\n",
    "for item in ds:\n",
    "    img = item['image']\n",
    "    print(type(img))\n",
    "    array = img.numpy()\n",
    "    print(array)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:38:39.570295Z",
     "start_time": "2023-06-10T20:38:39.554626Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.src.layers import StringLookup\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T19:52:45.802238Z",
     "start_time": "2023-06-10T19:52:45.796617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_path = '../../data/preprocessed_data/train'\n",
    "vocab_path = '../../data/preprocessed_data/vocab.txt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T19:52:33.397550Z",
     "start_time": "2023-06-10T19:52:33.392829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 17768592637663142582\n"
     ]
    }
   ],
   "source": [
    "decoder_vocab = []\n",
    "with open(vocab_path, 'r') as f:\n",
    "    for line in f:\n",
    "        decoder_vocab.append(line.strip())\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.load(train_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:18:42.035137Z",
     "start_time": "2023-06-10T20:18:41.989204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# gets PIL image and returns augmented PIL image\n",
    "def augment_img(img: tf.Tensor):\n",
    "    # only augment 3/4th the images\n",
    "    if random.randint(1, 4) > 3:\n",
    "        return img\n",
    "    print(type(img))\n",
    "    img = tf.make_ndarray(img)\n",
    "    # img = img.numpy()\n",
    "    # img = np.asarray(img)  # convert to numpy for opencv\n",
    "    print(img.dtype)\n",
    "\n",
    "    # morphological alterations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    if random.randint(1, 5) == 1:\n",
    "        # dilation because the image is not inverted\n",
    "        img = cv2.erode(img, kernel, iterations=random.randint(1, 2))\n",
    "    if random.randint(1, 6) == 1:\n",
    "        # erosion because the image is not inverted\n",
    "        img = cv2.dilate(img, kernel, iterations=random.randint(1, 1))\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    # noise introduction\n",
    "    transform = A.Compose([\n",
    "\n",
    "        A.OneOf([\n",
    "            # add black pixels noise\n",
    "            A.OneOf([\n",
    "                # A.RandomRain(brightness_coefficient=1.0, drop_length=2, drop_width=2, drop_color = (0, 0, 0), blur_value=1, rain_type = 'drizzle', p=0.05),\n",
    "                #  A.RandomShadow(p=1),\n",
    "                A.PixelDropout(p=1),\n",
    "            ], p=0.9),\n",
    "\n",
    "            # add white pixels noise\n",
    "            A.OneOf([\n",
    "                A.RandomRain(brightness_coefficient=1.0, drop_length=2,\n",
    "                             drop_width=2, drop_color=(255, 255, 255),\n",
    "                             blur_value=1, rain_type=None, p=1),\n",
    "            ], p=0.9),\n",
    "        ], p=1),\n",
    "\n",
    "        # transformations\n",
    "        A.OneOf([\n",
    "            A.ShiftScaleRotate(shift_limit=0, scale_limit=0.25, rotate_limit=2,\n",
    "                               border_mode=cv2.BORDER_CONSTANT,\n",
    "                               value=(255, 255, 255), p=1),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=8,\n",
    "                               border_mode=cv2.BORDER_CONSTANT,\n",
    "                               value=(255, 255, 255), p=1),\n",
    "            A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.15,\n",
    "                               rotate_limit=11, border_mode=cv2.BORDER_CONSTANT,\n",
    "                               value=(255, 255, 255), p=1),\n",
    "            A.Affine(shear=random.randint(-5, 5), mode=cv2.BORDER_CONSTANT,\n",
    "                     cval=(255, 255, 255), p=1),\n",
    "            A.ElasticTransform(alpha=random.randint(0, 10),\n",
    "                               sigma=random.randint(0, 100)),\n",
    "        ], p=0.5),\n",
    "        # A.Blur(blur_limit=5,p=0.25),\n",
    "    ])\n",
    "    img = transform(image=img)\n",
    "\n",
    "    img = tf.convert_to_tensor(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# def augment_using_ops(images, labels):\n",
    "# \t# randomly flip the images horizontally, randomly flip the images\n",
    "# \t# vertically, and rotate the images by 90 degrees in the counter\n",
    "# \t# clockwise direction\n",
    "# \timages = tf.image.random_flip_left_right(images)\n",
    "# \timages = tf.image.random_flip_up_down(images)\n",
    "# \timages = tf.image.rot90(images)\n",
    "# \t# return the image and the label\n",
    "# \treturn (images, labels)\n",
    "\n",
    "# def load_batch():\n",
    "#     ds = tf.data.Dataset.from_tensor_slices(imagePaths)\n",
    "#     ds = (ds\n",
    "#           # .shuffle(len(imagePaths), seed=42)\n",
    "#           # .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "#           # .cache()\n",
    "#           # .batch(BATCH_SIZE)\n",
    "#           # call the augmentation method here\n",
    "#           .map(augment_img, num_parallel_calls=AUTOTUNE)\n",
    "#           # .prefetch(tf.data.AUTOTUNE)\n",
    "#           )\n",
    "#\n",
    "#     return ds\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:48:00.366748Z",
     "start_time": "2023-06-10T20:48:00.363411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "from util.global_params import *\n",
    "\n",
    "# from data_augmentation import augment_img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:48:04.164453Z",
     "start_time": "2023-06-10T20:48:04.145117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.take_op._TakeDataset'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 17768592637663142582\n",
      "2023-06-10 22:48:11.552777: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ds = train_ds.take(1)\n",
    "print(type(ds))\n",
    "for item in ds:\n",
    "    img = item['image']\n",
    "    print(type(img))\n",
    "    array = img.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:48:11.557613Z",
     "start_time": "2023-06-10T20:48:11.465355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreeaioanatudor/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 17768592637663142582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/ipykernel_55951/644587297.py\", line 14, in None  *\n        lambda x: {'image': augment_img(x['image']), 'label': x['label']}\n    File \"/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/ipykernel_55951/2922131686.py\", line 15, in augment_img  *\n        img = tf.make_ndarray(img)\n\n    AttributeError: 'Tensor' object has no attribute 'tensor_shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[115], line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m train_ds\u001B[38;5;241m.\u001B[39mbatch(BATCH_SIZE)\u001B[38;5;241m.\u001B[39mcache()\u001B[38;5;241m.\u001B[39mprefetch(\n\u001B[1;32m      5\u001B[0m         buffer_size\u001B[38;5;241m=\u001B[39mAUTOTUNE)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Calculate the length of the batch before augmentation\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# batch_length_before = tf.data.experimental.cardinality(\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#     train_ds).numpy()\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# print(\"Batch Length Before Augmentation:\", batch_length_before)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# augment the data\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m augmented_images_batches \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment_img\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAUTOTUNE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# train_batches = train_ds.concatenate(augmented_images_batches)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# # Calculate the length of the batch after augmentation\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# batch_length_after = tf.data.experimental.cardinality(\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#     train_batches).numpy() * BATCH_SIZE\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# print(\"Batch Length After Augmentation:\", batch_length_after)\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001B[0m, in \u001B[0;36mDatasetV2.map\u001B[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[1;32m   2274\u001B[0m \u001B[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001B[39;00m\n\u001B[1;32m   2275\u001B[0m \u001B[38;5;66;03m# dataset_ops).\u001B[39;00m\n\u001B[1;32m   2276\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001B[39;00m\n\u001B[1;32m   2277\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m map_op\n\u001B[0;32m-> 2278\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmap_op\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_v2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2279\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2280\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2281\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2282\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2283\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:40\u001B[0m, in \u001B[0;36m_map_v2\u001B[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[1;32m     37\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _MapDataset(\n\u001B[1;32m     38\u001B[0m       input_dataset, map_func, preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 40\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ParallelMapDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m      \u001B[49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m      \u001B[49m\u001B[43mpreserve_cardinality\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:148\u001B[0m, in \u001B[0;36m_ParallelMapDataset.__init__\u001B[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_dataset \u001B[38;5;241m=\u001B[39m input_dataset\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_inter_op_parallelism \u001B[38;5;241m=\u001B[39m use_inter_op_parallelism\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func \u001B[38;5;241m=\u001B[39m \u001B[43mstructured_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructuredFunctionWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transformation_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_legacy_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_legacy_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m deterministic \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deterministic \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:272\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__\u001B[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[1;32m    265\u001B[0m       warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    266\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    267\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moption is set, this option does not apply to tf.data functions. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    268\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo force eager execution of tf.data functions, please use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    269\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    270\u001B[0m     fn_factory \u001B[38;5;241m=\u001B[39m trace_tf_function(defun_kwargs)\n\u001B[0;32m--> 272\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function \u001B[38;5;241m=\u001B[39m \u001B[43mfn_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;66;03m# There is no graph to add in eager mode.\u001B[39;00m\n\u001B[1;32m    274\u001B[0m add_to_graph \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly()\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1189\u001B[0m, in \u001B[0;36mFunction.get_concrete_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_concrete_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1188\u001B[0m   \u001B[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001B[39;00m\n\u001B[0;32m-> 1189\u001B[0m   concrete \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_concrete_function_garbage_collected\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1190\u001B[0m   concrete\u001B[38;5;241m.\u001B[39m_garbage_collector\u001B[38;5;241m.\u001B[39mrelease()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m concrete\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1169\u001B[0m, in \u001B[0;36mFunction._get_concrete_function_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1167\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1168\u001B[0m     initializers \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m-> 1169\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_initializers_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitializers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1170\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialize_uninitialized_variables(initializers)\n\u001B[1;32m   1172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m   1173\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m   1174\u001B[0m   \u001B[38;5;66;03m# version which is guaranteed to never create variables.\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001B[0m, in \u001B[0;36mFunction._initialize\u001B[0;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;66;03m# Force the definition of the function for these arguments\u001B[39;00m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_variable_creation_fn \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 694\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_fn\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_concrete_function_internal_garbage_collected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvalid_creator_scope\u001B[39m(\u001B[38;5;241m*\u001B[39munused_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwds):\n\u001B[1;32m    699\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:176\u001B[0m, in \u001B[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m--> 176\u001B[0m   concrete_function, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_concrete_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001B[0m, in \u001B[0;36mTracingCompiler._maybe_define_concrete_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m   args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_signature\n\u001B[1;32m    169\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:398\u001B[0m, in \u001B[0;36mTracingCompiler._maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m    395\u001B[0m args \u001B[38;5;241m=\u001B[39m placeholder_bound_args\u001B[38;5;241m.\u001B[39margs\n\u001B[1;32m    396\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m placeholder_bound_args\u001B[38;5;241m.\u001B[39mkwargs\n\u001B[0;32m--> 398\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_concrete_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc_graph\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;66;03m# TODO(b/263520817): Remove access to private attribute.\u001B[39;00m\n\u001B[1;32m    402\u001B[0m graph_capture_container \u001B[38;5;241m=\u001B[39m concrete_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mfunction_captures\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:305\u001B[0m, in \u001B[0;36mTracingCompiler._create_concrete_function\u001B[0;34m(self, args, kwargs, func_graph)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    302\u001B[0m   arg_names \u001B[38;5;241m=\u001B[39m base_arg_names\n\u001B[1;32m    304\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m monomorphic_function\u001B[38;5;241m.\u001B[39mConcreteFunction(\n\u001B[0;32m--> 305\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_placeholders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m,\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[1;32m    316\u001B[0m     spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[1;32m    321\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1055\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001B[0m\n\u001B[1;32m   1052\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[1;32m   1054\u001B[0m _, original_func \u001B[38;5;241m=\u001B[39m tf_decorator\u001B[38;5;241m.\u001B[39munwrap(python_func)\n\u001B[0;32m-> 1055\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpython_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfunc_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfunc_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;66;03m# TensorArrays and `None`s.\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m variable_utils\u001B[38;5;241m.\u001B[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001B[0m, in \u001B[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m default_graph\u001B[38;5;241m.\u001B[39m_variable_creator_scope(scope, priority\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m):  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    594\u001B[0m   \u001B[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001B[39;00m\n\u001B[1;32m    595\u001B[0m   \u001B[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001B[39;00m\n\u001B[1;32m    596\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(compile_with_xla):\n\u001B[0;32m--> 597\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mweak_wrapped_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__wrapped__\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    598\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:238\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_fn\u001B[39m(\u001B[38;5;241m*\u001B[39margs):  \u001B[38;5;66;03m# pylint: disable=missing-docstring\u001B[39;00m\n\u001B[0;32m--> 238\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mwrapper_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m   ret \u001B[38;5;241m=\u001B[39m structure\u001B[38;5;241m.\u001B[39mto_tensor_list(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_structure, ret)\n\u001B[1;32m    240\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m [ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m ret]\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:168\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _should_unpack(nested_args):\n\u001B[1;32m    167\u001B[0m   nested_args \u001B[38;5;241m=\u001B[39m (nested_args,)\n\u001B[0;32m--> 168\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mautograph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtf_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag_ctx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnested_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m ret \u001B[38;5;241m=\u001B[39m variable_utils\u001B[38;5;241m.\u001B[39mconvert_variables_to_tensors(ret)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _should_pack(ret):\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:693\u001B[0m, in \u001B[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m    692\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 693\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m    694\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    695\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:690\u001B[0m, in \u001B[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    688\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    689\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m conversion_ctx:\n\u001B[0;32m--> 690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m    692\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001B[0m, in \u001B[0;36mconverted_call\u001B[0;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    438\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mconverted_f\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43meffective_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    440\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    441\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args)\n",
      "File \u001B[0;32m/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/__autograph_generated_file7ody88oi.py:5\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner_factory\u001B[39m(ag__):\n\u001B[0;32m----> 5\u001B[0m     tf__lam \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwith_function_scope\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlscope\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugment_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlscope\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlscope\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSTD\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf__lam\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/core/function_wrappers.py:113\u001B[0m, in \u001B[0;36mwith_function_scope\u001B[0;34m(thunk, scope_name, options)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m FunctionScope(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda_\u001B[39m\u001B[38;5;124m'\u001B[39m, scope_name, options) \u001B[38;5;28;01mas\u001B[39;00m scope:\n\u001B[0;32m--> 113\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mthunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscope\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/__autograph_generated_file7ody88oi.py:5\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001B[0;34m(lscope)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner_factory\u001B[39m(ag__):\n\u001B[0;32m----> 5\u001B[0m     tf__lam \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: ag__\u001B[38;5;241m.\u001B[39mwith_function_scope(\u001B[38;5;28;01mlambda\u001B[39;00m lscope: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugment_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlscope\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]}, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlscope\u001B[39m\u001B[38;5;124m'\u001B[39m, ag__\u001B[38;5;241m.\u001B[39mSTD)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf__lam\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:441\u001B[0m, in \u001B[0;36mconverted_call\u001B[0;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[1;32m    439\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    440\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 441\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mconverted_f\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43meffective_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    443\u001B[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001B[0;32m/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/__autograph_generated_files_hfjh4a.py:77\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__augment_img\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     75\u001B[0m transform \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefined(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     76\u001B[0m kernel \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefined(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkernel\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 77\u001B[0m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mif_stmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mif_body_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43melse_body_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_state_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_state_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdo_return\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mretval_\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fscope\u001B[38;5;241m.\u001B[39mret(retval_, do_return)\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1269\u001B[0m, in \u001B[0;36mif_stmt\u001B[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001B[0m\n\u001B[1;32m   1267\u001B[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001B[1;32m   1268\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1269\u001B[0m   \u001B[43m_py_if_stmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morelse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1322\u001B[0m, in \u001B[0;36m_py_if_stmt\u001B[0;34m(cond, body, orelse)\u001B[0m\n\u001B[1;32m   1320\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_py_if_stmt\u001B[39m(cond, body, orelse):\n\u001B[1;32m   1321\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m body() \u001B[38;5;28;01mif\u001B[39;00m cond \u001B[38;5;28;01melse\u001B[39;00m \u001B[43morelse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/__autograph_generated_files_hfjh4a.py:30\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__augment_img.<locals>.else_body_2\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mnonlocal\u001B[39;00m img, do_return, retval_\n\u001B[1;32m     29\u001B[0m ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mprint\u001B[39m)(ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mtype\u001B[39m), (ag__\u001B[38;5;241m.\u001B[39mld(img),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope))\n\u001B[0;32m---> 30\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_ndarray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mprint\u001B[39m)(ag__\u001B[38;5;241m.\u001B[39mld(img)\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m     32\u001B[0m kernel \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(cv2)\u001B[38;5;241m.\u001B[39mgetStructuringElement, (ag__\u001B[38;5;241m.\u001B[39mld(cv2)\u001B[38;5;241m.\u001B[39mMORPH_ELLIPSE, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001B[0m, in \u001B[0;36mconverted_call\u001B[0;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[1;32m    374\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _call_unconverted(f, args, kwargs, options)\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39muser_requested \u001B[38;5;129;01mand\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_allowlisted(f):\n\u001B[0;32m--> 377\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001B[39;00m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001B[39;00m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# things like builtins.\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39minternal_convert_user_code:\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[0;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    459\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 460\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/framework/tensor_util.py:656\u001B[0m, in \u001B[0;36mMakeNdarray\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mMakeNdarray\u001B[39m(tensor):\n\u001B[1;32m    631\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Create a numpy ndarray from a tensor.\u001B[39;00m\n\u001B[1;32m    632\u001B[0m \n\u001B[1;32m    633\u001B[0m \u001B[38;5;124;03m  Create a numpy ndarray with the same shape and data as the tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    654\u001B[0m \n\u001B[1;32m    655\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 656\u001B[0m   shape \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39msize \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor_shape\u001B[49m\u001B[38;5;241m.\u001B[39mdim]\n\u001B[1;32m    657\u001B[0m   num_elements \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mprod(shape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[1;32m    658\u001B[0m   tensor_dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(tensor\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m~/Desktop/Uni/Msc/HWR/Handwriting-Recognition/english_handwriting/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:430\u001B[0m, in \u001B[0;36mTensor.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mastype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mravel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranspose\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreshape\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclip\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    422\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtolist\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m    423\u001B[0m   \u001B[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001B[39;00m\n\u001B[1;32m    424\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m    425\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001B[39m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001B[39m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;124m    np_config.enable_numpy_behavior()\u001B[39m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;124m  \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m--> 430\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: in user code:\n\n    File \"/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/ipykernel_55951/644587297.py\", line 14, in None  *\n        lambda x: {'image': augment_img(x['image']), 'label': x['label']}\n    File \"/var/folders/3d/1fsp6_k10ng99vqrbkbg8s100000gn/T/ipykernel_55951/2922131686.py\", line 15, in augment_img  *\n        img = tf.make_ndarray(img)\n\n    AttributeError: 'Tensor' object has no attribute 'tensor_shape'\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "train_ds = tf.data.Dataset.load(train_path)\n",
    "train_ds = train_ds.batch(BATCH_SIZE).cache().prefetch(\n",
    "        buffer_size=AUTOTUNE)\n",
    "\n",
    "# Calculate the length of the batch before augmentation\n",
    "# batch_length_before = tf.data.experimental.cardinality(\n",
    "#     train_ds).numpy()\n",
    "# print(\"Batch Length Before Augmentation:\", batch_length_before)\n",
    "\n",
    "# augment the data\n",
    "augmented_images_batches = train_ds.map(\n",
    "    lambda x: {'image': augment_img(x['image']), 'label': x['label']},\n",
    "    num_parallel_calls=AUTOTUNE)\n",
    "# train_batches = train_ds.concatenate(augmented_images_batches)\n",
    "# # Calculate the length of the batch after augmentation\n",
    "# batch_length_after = tf.data.experimental.cardinality(\n",
    "#     train_batches).numpy() * BATCH_SIZE\n",
    "# print(\"Batch Length After Augmentation:\", batch_length_after)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:48:23.224858Z",
     "start_time": "2023-06-10T20:48:22.790755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "decoder_vocab[1] = ' '\n",
    "decoder = StringLookup(vocabulary=decoder_vocab,\n",
    "                           mask_token=None, invert=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:22:43.551330Z",
     "start_time": "2023-06-10T20:22:43.543902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augmented_images_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[43maugmented_images_batches\u001B[49m\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# batch_length = tf.data.experimental.cardinality(\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#     data).numpy()\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# print(batch_length)\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m], data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# print(decoder_vocab)\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'augmented_images_batches' is not defined"
     ]
    }
   ],
   "source": [
    "for data in augmented_images_batches.take(1):\n",
    "    # batch_length = tf.data.experimental.cardinality(\n",
    "    #     data).numpy()\n",
    "    # print(batch_length)\n",
    "\n",
    "    images, labels = data['image'], data['label']\n",
    "\n",
    "    for i in range(4):\n",
    "        # print(decoder_vocab)\n",
    "        image = images[i]\n",
    "        label = labels[i]\n",
    "        image = tf.keras.preprocessing.image.array_to_img(image)\n",
    "\n",
    "        print(label)\n",
    "        # Display the image using Matplotlib\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, 99)))\n",
    "        # Convert to string.\n",
    "        label = tf.strings.reduce_join(decoder(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "        plt.title(label)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T20:24:54.178923Z",
     "start_time": "2023-06-10T20:24:54.047869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
